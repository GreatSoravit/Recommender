{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cvTdMyXdODex"
   },
   "source": [
    "# Part-Pre. Preparation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ww--_kl9-ndn"
   },
   "source": [
    "## Pre 1. Setup Block\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iFgYpbhh0tkX",
    "outputId": "5dfcb384-7ecd-4de4-fa7a-c12faa71c3f2"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'rm' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      " 33 7631k   33 2587k    0     0  5033k      0  0:00:01 --:--:--  0:00:01 5023k\n",
      "100 7631k  100 7631k    0     0  8516k      0 --:--:-- --:--:-- --:--:-- 8516k\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "100 2366k  100 2366k    0     0  6130k      0 --:--:-- --:--:-- --:--:-- 6130k\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "  3 7581k    3  234k    0     0  1394k      0  0:00:05 --:--:--  0:00:05 1394k\n",
      "100 7581k  100 7581k    0     0  10.0M      0 --:--:-- --:--:-- --:--:-- 10.0M\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "100 1895k  100 1895k    0     0  5743k      0 --:--:-- --:--:-- --:--:-- 5743k\n"
     ]
    }
   ],
   "source": [
    "!rm -rf ratings* books* to_read* test*\n",
    "\n",
    "!curl -o ratings.csv \"http://www.dcs.gla.ac.uk/~craigm/recsysH/coursework/final-ratings.csv\" \n",
    "!curl -o books.csv \"http://www.dcs.gla.ac.uk/~craigm/recsysH/coursework/final-books.csv\"\n",
    "!curl -o to_read.csv \"http://www.dcs.gla.ac.uk/~craigm/recsysH/coursework/final-to_read.csv\"\n",
    "!curl -o test.csv \"http://www.dcs.gla.ac.uk/~craigm/recsysH/coursework/final-test.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1VpVnNrZ1EiX",
    "outputId": "4b67c24b-ab4c-45da-ad5e-8edc1327991b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spotlight from git+https://github.com/cmacdonald/spotlight.git@master#egg=spotlight in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (0.1.6)\n",
      "Requirement already satisfied: torch>=0.4.0 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from spotlight) (1.8.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from torch>=0.4.0->spotlight) (1.19.5)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from torch>=0.4.0->spotlight) (3.7.4.3)\n"
     ]
    }
   ],
   "source": [
    "#Standard setup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "!pip install git+https://github.com/cmacdonald/spotlight.git@master#egg=spotlight\n",
    "from spotlight.interactions import Interactions\n",
    "SEED=20\n",
    "BPRMF=None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RtJO0e0m-hun"
   },
   "source": [
    "## Pre 2. Data Preparation\n",
    "\n",
    "Let's load the dataset into dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "qKAb25iw1MYw"
   },
   "outputs": [],
   "source": [
    "#load in the csv files\n",
    "ratings_df = pd.read_csv(\"ratings.csv\")\n",
    "books_df = pd.read_csv(\"books.csv\")\n",
    "to_read_df = pd.read_csv(\"to_read.csv\")\n",
    "test = pd.read_csv(\"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>user_id</th>\n",
       "      <th>book_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>560054</td>\n",
       "      <td>2278</td>\n",
       "      <td>4232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>277900</td>\n",
       "      <td>2118</td>\n",
       "      <td>298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>87083</td>\n",
       "      <td>2769</td>\n",
       "      <td>3934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>77727</td>\n",
       "      <td>2540</td>\n",
       "      <td>293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>240676</td>\n",
       "      <td>3142</td>\n",
       "      <td>147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>489415</th>\n",
       "      <td>495742</td>\n",
       "      <td>694</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>489416</th>\n",
       "      <td>511918</td>\n",
       "      <td>3489</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>489417</th>\n",
       "      <td>151565</td>\n",
       "      <td>4296</td>\n",
       "      <td>316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>489418</th>\n",
       "      <td>248285</td>\n",
       "      <td>2655</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>489419</th>\n",
       "      <td>133502</td>\n",
       "      <td>3243</td>\n",
       "      <td>1365</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>489420 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Unnamed: 0  user_id  book_id\n",
       "0           560054     2278     4232\n",
       "1           277900     2118      298\n",
       "2            87083     2769     3934\n",
       "3            77727     2540      293\n",
       "4           240676     3142      147\n",
       "...            ...      ...      ...\n",
       "489415      495742      694        3\n",
       "489416      511918     3489       61\n",
       "489417      151565     4296      316\n",
       "489418      248285     2655       46\n",
       "489419      133502     3243     1365\n",
       "\n",
       "[489420 rows x 3 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_read_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "W6rqfn53OhDC"
   },
   "outputs": [],
   "source": [
    "#cut down the number of items and users\n",
    "counts=ratings_df[ratings_df[\"book_id\"] < 2000].groupby([\"book_id\"]).count().reset_index()\n",
    "valid_books=counts[counts[\"user_id\"] >= 10][[\"book_id\"]]\n",
    "\n",
    "books_df = books_df.merge(valid_books, on=\"book_id\")\n",
    "ratings_df = ratings_df[ratings_df[\"user_id\"] < 2000].merge(valid_books, on=\"book_id\")\n",
    "to_read_df = to_read_df[to_read_df[\"user_id\"] < 2000].merge(valid_books, on=\"book_id\")\n",
    "test = test[test[\"user_id\"] < 2000].merge(valid_books, on=\"book_id\")\n",
    "\n",
    "\n",
    "#stringify the id columns\n",
    "def str_col(df):\n",
    "  if \"user_id\" in df.columns:\n",
    "    df[\"user_id\"] = \"u\" + df.user_id.astype(str)\n",
    "  if \"book_id\" in df.columns:\n",
    "    df[\"book_id\"] = \"b\" + df.book_id.astype(str)\n",
    "\n",
    "str_col(books_df)\n",
    "str_col(ratings_df)\n",
    "str_col(to_read_df)\n",
    "str_col(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "15ClgJOdTTt1",
    "outputId": "70aee8fc-533c-40ea-c306-78015e56b5de"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Interactions dataset (1999 users x 1826 items x 124762 interactions)>\n",
      "<Interactions dataset (1999 users x 1826 items x 135615 interactions)>\n",
      "<Interactions dataset (1999 users x 1826 items x 33917 interactions)>\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "from itertools import count\n",
    "\n",
    "from spotlight.cross_validation import random_train_test_split\n",
    "\n",
    "iid_map = defaultdict(count().__next__)\n",
    "\n",
    "\n",
    "rating_iids = np.array([iid_map[iid] for iid in ratings_df[\"book_id\"].values], dtype = np.int32)\n",
    "test_iids = np.array([iid_map[iid] for iid in test[\"book_id\"].values], dtype = np.int32)\n",
    "toread_iids = np.array([iid_map[iid] for iid in to_read_df[\"book_id\"].values], dtype = np.int32)\n",
    "\n",
    "\n",
    "uid_map = defaultdict(count().__next__)\n",
    "test_uids = np.array([uid_map[uid] for uid in test[\"user_id\"].values], dtype = np.int32)\n",
    "rating_uids = np.array([uid_map[uid] for uid in ratings_df[\"user_id\"].values], dtype = np.int32)\n",
    "toread_uids = np.array([uid_map[iid] for iid in to_read_df[\"user_id\"].values], dtype = np.int32)\n",
    "\n",
    "\n",
    "uid_rev_map = {v: k for k, v in uid_map.items()}\n",
    "iid_rev_map = {v: k for k, v in iid_map.items()}\n",
    "\n",
    "\n",
    "rating_dataset = Interactions(user_ids=rating_uids,\n",
    "                               item_ids=rating_iids,\n",
    "                               ratings=ratings_df[\"rating\"].values,\n",
    "                               num_users=len(uid_rev_map),\n",
    "                               num_items=len(iid_rev_map))\n",
    "\n",
    "toread_dataset = Interactions(user_ids=toread_uids,\n",
    "                               item_ids=toread_iids,\n",
    "                               num_users=len(uid_rev_map),\n",
    "                               num_items=len(iid_rev_map))\n",
    "\n",
    "test_dataset = Interactions(user_ids=test_uids,\n",
    "                               item_ids=test_iids,\n",
    "                               num_users=len(uid_rev_map),\n",
    "                               num_items=len(iid_rev_map))\n",
    "\n",
    "print(rating_dataset)\n",
    "print(toread_dataset)\n",
    "print(test_dataset)\n",
    "\n",
    "# define the validation set\n",
    "toread_dataset_train, validation = random_train_test_split(toread_dataset, random_state=np.random.RandomState(SEED))\n",
    "\n",
    "num_items = test_dataset.num_items\n",
    "num_users = test_dataset.num_users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2kDxZgICBFp6",
    "outputId": "884f4df0-a738-4968-f365-ef5d4d61119e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iid 0: Carlos Ruiz Zafón, Lucia Graves / The Shadow of the Wind (The Cemetery of Forgotten Books,  #1)\n"
     ]
    }
   ],
   "source": [
    "def getAuthorTitle(iid):\n",
    "  bookid = iid_rev_map[iid]\n",
    "  row = books_df[books_df.book_id == bookid]\n",
    "  return row.iloc[0][\"authors\"] + \" / \" + row.iloc[0][\"title\"]\n",
    "\n",
    "print(\"iid 0: \" + getAuthorTitle(0) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kt4I2C5DTUL5"
   },
   "source": [
    "## Pre 3. Example Code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "s2eaxy_hakbC",
    "outputId": "3b1a9e39-86ac-45b3-afea-f5b904795036"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "from spotlight.evaluation import mrr_score, precision_recall_score\n",
    "\n",
    "class dummymodel:\n",
    "  \n",
    "  def __init__(self, numitems):\n",
    "    self.predictions=np.zeros(numitems)\n",
    "  \n",
    "  #uid is the user we are requesting recommendations for;\n",
    "  #returns an array of scores, one for each item\n",
    "  def predict(self, uid):\n",
    "    #this model returns all zeros, regardless of userid\n",
    "    return( self.predictions )\n",
    "\n",
    "#lets evaluate how the effeciveness of dummymodel\n",
    "\n",
    "print(mrr_score(dummymodel(num_items), test_dataset, train=rating_dataset, k=100).mean())\n",
    "#as expected, a recommendation model that gives 0 scores for all items obtains a MRR score of 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZQTJOmS5dB3i",
    "outputId": "ad02f813-9f6b-40c0-f2fe-f14f4932f658"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1999it [00:00, 4680.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#note that mrr_score() displays a progress bar if you set verbose=True\n",
    "print(mrr_score(dummymodel(num_items), test_dataset, train=rating_dataset, k=100, verbose=True).mean())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BCWXwVC5Mtyj"
   },
   "source": [
    "# Part-A. Combination of Recommendation Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SyvGgW_3ZjLV"
   },
   "source": [
    "## 1. Explicit & Implicit Matrix Factorisation Models\n",
    "\n",
    "Create and train three matrix factorisation systems:\n",
    " - \"EMF\": explicit MF, trained on the ratings Interactions object (`rating_dataset`)\n",
    " - \"IMF\": implicit MF, trained on the toread_dataset Interactions object (`toread_dataset_train`)\n",
    " - \"BPRMF\": implicit MF with the BPR loss function (`loss='bpr'`), trained on the toread_dataset Interactions object (`toread_dataset_train`)\n",
    "  \n",
    "In all cases, use the standard initialisation arguments,\n",
    "`n_iter=10, embedding_dim=32, use_cuda=False, random_state=np.random.RandomState(SEED)`.\n",
    " \n",
    "Evaluate each of these models in terms of Mean Reciprocal Rank on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qDADjtepRvpJ",
    "outputId": "5c52b3f8-ed0a-4570-e60a-923ddc3e80bf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: loss 3.8710271519471386\n",
      "Epoch 1: loss 0.7940810433909541\n",
      "Epoch 2: loss 0.6382512596176296\n",
      "Epoch 3: loss 0.5217335244304822\n",
      "Epoch 4: loss 0.4484485583837892\n",
      "Epoch 5: loss 0.4054335061399663\n",
      "Epoch 6: loss 0.38238631036193643\n",
      "Epoch 7: loss 0.36336619852751983\n",
      "Epoch 8: loss 0.351379360576145\n",
      "Epoch 9: loss 0.33966925004344495\n",
      "Training took 18 seconds\n",
      "Epoch 0: loss 0.7356257028174851\n",
      "Epoch 1: loss 0.5271942592454406\n",
      "Epoch 2: loss 0.46312756476537237\n",
      "Epoch 3: loss 0.42142992773146\n",
      "Epoch 4: loss 0.3908996549979696\n",
      "Epoch 5: loss 0.3679050632805195\n",
      "Epoch 6: loss 0.34888975125438765\n",
      "Epoch 7: loss 0.33679830007395656\n",
      "Epoch 8: loss 0.3223336052219823\n",
      "Epoch 9: loss 0.312420713564135\n",
      "Training took 34 seconds\n",
      "Epoch 0: loss 0.31877833036881575\n",
      "Epoch 1: loss 0.18799531937770123\n",
      "Epoch 2: loss 0.15685450027011474\n",
      "Epoch 3: loss 0.14052333286348379\n",
      "Epoch 4: loss 0.12967147759671482\n",
      "Epoch 5: loss 0.12273261361526994\n",
      "Epoch 6: loss 0.11806230488813148\n",
      "Epoch 7: loss 0.1144794372033398\n",
      "Epoch 8: loss 0.11053412522206892\n",
      "Epoch 9: loss 0.10809499274166125\n",
      "Training took 35 seconds\n"
     ]
    }
   ],
   "source": [
    "# Add your solution here\n",
    "\n",
    "#solution goes here\n",
    "\n",
    "from spotlight.factorization.explicit import ExplicitFactorizationModel\n",
    "from spotlight.factorization.implicit import ImplicitFactorizationModel\n",
    "import time  \n",
    "\n",
    "EMF = ExplicitFactorizationModel(n_iter=10, \n",
    "                                    embedding_dim=32, #this is Spotlight default\n",
    "                                    use_cuda=False,\n",
    "                                    random_state=np.random.RandomState(SEED) # ensure results are repeatable\n",
    ")\n",
    "current = time.time()\n",
    "\n",
    "EMF.fit(rating_dataset, verbose=True)\n",
    "end = time.time()\n",
    "diff = end - current\n",
    "print(\"Training took %d seconds\" % (diff))\n",
    "\n",
    "IMF = ImplicitFactorizationModel(n_iter=10, \n",
    "                                    embedding_dim=32, #this is Spotlight default\n",
    "                                    use_cuda=False,\n",
    "                                    random_state=np.random.RandomState(SEED) # ensure results are repeatable\n",
    ")\n",
    "current = time.time()\n",
    "\n",
    "IMF.fit(toread_dataset, verbose=True)\n",
    "end = time.time()\n",
    "diff = end - current\n",
    "print(\"Training took %d seconds\" % (diff))\n",
    "\n",
    "BPRMF = ImplicitFactorizationModel(n_iter=10, \n",
    "                                    embedding_dim=32, #this is Spotlight default\n",
    "                                    use_cuda=False,\n",
    "                                    loss='bpr',\n",
    "                                    random_state=np.random.RandomState(SEED) # ensure results are repeatable\n",
    ")\n",
    "current = time.time()\n",
    "\n",
    "BPRMF.fit(toread_dataset, verbose=True)\n",
    "end = time.time()\n",
    "diff = end - current\n",
    "print(\"Training took %d seconds\" % (diff))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b_zeqroniUN4",
    "outputId": "c1fe85f5-f1e8-4195-c233-9be95351eecc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MRR Score of EMF: 0.05898399982013507\n",
      "MRR Score of IMF: 0.32955393223971\n",
      "MRR Score of BPRMF: 0.41691597413244963\n"
     ]
    }
   ],
   "source": [
    "print(\"MRR Score of EMF:\", mrr_score(EMF, test_dataset, train=rating_dataset, k=100, verbose=False).mean())\n",
    "print(\"MRR Score of IMF:\", mrr_score(IMF, test_dataset, train=rating_dataset, k=100, verbose=False).mean())\n",
    "print(\"MRR Score of BPRMF:\", mrr_score(BPRMF, test_dataset, train=rating_dataset, k=100, verbose=False).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hZHCOmfEDOGo"
   },
   "source": [
    "## Task 2. Hybrid Model\n",
    "\n",
    "(a) Linearly combine the *scores* from IMF and BPRMF.  Normalise both input scores into the range 0..1 using [sklearn's minmax_scale() function](\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.minmax_scale.html) before combining them.\n",
    "\n",
    "(b) Apply a pipelining recommender, where the top 100 items are obtained from IMF and re-ranked using the scores of BPRMF. Items not returned by IMF get a score of 0.\n",
    "\n",
    "To implement these hybrid models, you should create new classes that abide by the Spotlight model contract (namely, it has a `predict(self, uid)` function that returns a score for *all* items). \n",
    "\n",
    "Evaluate each model in terms of MRR. How many users are improved, how many are degraded compared to the BPRMF baseline?\n",
    "\n",
    "Finally, pass your instantiated model object to the `test_Hybrid_a()` (for (a)) or `test_Hybrid_b()` (for (b)) functions, as appropriate, and record the results in the quiz. For example, if your model for (b) is called `pipeline`, then you would run:\n",
    "```python\n",
    "test_Hybrid_b(pipeline)\n",
    "``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "6j6JzIOHkYw9"
   },
   "outputs": [],
   "source": [
    "def test_Hybrid_a(combsumObj):\n",
    "  for i, u in enumerate([5, 20]):\n",
    "    print(\"Hybrid a test case %d\" % i)\n",
    "    print(np.count_nonzero(combsumObj.predict(u) > 1))\n",
    "\n",
    "def test_Hybrid_b(pipeObj):\n",
    "  for i, iid in enumerate([3, 0]):\n",
    "    print(\"Hybrid b test case %d\" % i)\n",
    "    print(pipeObj.predict(0)[iid])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "1_o7a1ppFZ7R"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import minmax_scale\n",
    "\n",
    "class HybridModelA: # Combine\n",
    "  def __init__(self, model_1, model_2):\n",
    "    self.model_1 = model_1 \n",
    "    self.model_2 = model_2\n",
    "\n",
    "  def predict(self, uid, iids=None):\n",
    "    pre_norm_m1 = minmax_scale(self.model_1.predict(uid, iids)) # predict model_1 then normalize\n",
    "    pre_norm_m2 = minmax_scale(self.model_2.predict(uid, iids)) # predict model_2 then normalize\n",
    "    return pre_norm_m1 + pre_norm_m2 # combine result from two models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "3ddLBrUFeZEw"
   },
   "outputs": [],
   "source": [
    "from scipy.stats import rankdata\n",
    "\n",
    "class HybridModelB: # Pipeline\n",
    "  def __init__(self, model_1, model_2):\n",
    "    self.model_1 = model_1 \n",
    "    self.model_2 = model_2\n",
    "\n",
    "  def predict(self, uid):\n",
    "    model_1_result = self.model_1.predict(uid) # Predict and get all value from IMF\n",
    "    rank_model_1 = rankdata(-model_1_result) # Get position and ranking \n",
    "    \n",
    "    #print(model_1_result[rank_model_1==rank_model_1.min()]) First rank high value\n",
    "    #print(model_1_result[rank_model_1==rank_model_1.max()]) Last rank lowest value\n",
    "    model_2_result = self.model_2.predict(uid) # Predict and get all value from BPRMF\n",
    "    model_2_result[np.argwhere(rank_model_1 > 100)] = 0 # Replace item of BPRMF with 0 by using rank position that not in top100\n",
    "\n",
    "    return model_2_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pcoSuYy1hoY5",
    "outputId": "ceebf8f4-2084-4933-8be7-252c66a31be5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MRR Score of BPRMF(Baseline): 0.41691597413244963\n",
      "--------------------------------------------------------\n",
      "MRR Score of Hybrid(IMF, BPRMF): 0.4158716450378546\n",
      "Improved users in MRR score between Hybrid and BPRMF: 731\n",
      "Degraded users in degraded in MRR score between Hybrid and to BPRMF: 737\n",
      "Not changed users in MRR score between Hybrid and to BPRMF: 531\n",
      "--------------------------------------------------------\n",
      "MRR Score of Hybrid Pipeline(IMF, BPRMF): 0.4205794627838617\n",
      "Improved users in MRR score between Pipeline and BPRMF: 584\n",
      "Degraded users in MRR score between Pipeline and to BPRMF: 184\n",
      "Not changed users in MRR score between Pipeline and to BPRMF: 1231\n"
     ]
    }
   ],
   "source": [
    "BPRMF_mrr_score = mrr_score(BPRMF, test_dataset, train=rating_dataset, k=100, verbose=False)\n",
    "print(\"MRR Score of BPRMF(Baseline):\", BPRMF_mrr_score.mean())\n",
    "print(\"--------------------------------------------------------\")\n",
    "\n",
    "Hybrid_A_mrr_score = mrr_score(HybridModelA(IMF, BPRMF), test_dataset, train=rating_dataset, k=100, verbose=False)\n",
    "print(\"MRR Score of Hybrid(IMF, BPRMF):\", Hybrid_A_mrr_score.mean())\n",
    "\n",
    "print(\"Improved users in MRR score between Hybrid and BPRMF:\", np.count_nonzero(BPRMF_mrr_score < Hybrid_A_mrr_score))\n",
    "print(\"Degraded users in degraded in MRR score between Hybrid and to BPRMF:\", np.count_nonzero(BPRMF_mrr_score > Hybrid_A_mrr_score))\n",
    "print(\"Not changed users in MRR score between Hybrid and to BPRMF:\", np.count_nonzero(BPRMF_mrr_score == Hybrid_A_mrr_score))\n",
    "\n",
    "print(\"--------------------------------------------------------\")\n",
    "\n",
    "Hybrid_B_mrr_score = mrr_score(HybridModelB(IMF, BPRMF), test_dataset, train=rating_dataset, k=100, verbose=False)\n",
    "print(\"MRR Score of Hybrid Pipeline(IMF, BPRMF):\", Hybrid_B_mrr_score.mean())\n",
    "\n",
    "print(\"Improved users in MRR score between Pipeline and BPRMF:\", np.count_nonzero(BPRMF_mrr_score < Hybrid_B_mrr_score))\n",
    "print(\"Degraded users in MRR score between Pipeline and to BPRMF:\", np.count_nonzero(BPRMF_mrr_score > Hybrid_B_mrr_score))\n",
    "print(\"Not changed users in MRR score between Pipeline and to BPRMF:\", np.count_nonzero(BPRMF_mrr_score == Hybrid_B_mrr_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qf0K6GECM0LQ"
   },
   "source": [
    "# Part-B. Analysing Recommendation Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-gszqk2kIZLX"
   },
   "source": [
    "## Utility methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xIyc_p_dIm1M",
    "outputId": "f7722ab8-2871-495a-a0d1-150091f395bb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Returned iids: [143 108  21  23  11  33 106  97  77  67]\n",
      "Returned scores: [1.         0.97905004 0.9210978  0.862887   0.85853875 0.8585235\n",
      " 0.8488022  0.84497577 0.83934355 0.83848405]\n",
      "Returned embeddings: tensor([[-1.0811,  0.6025,  2.5012, -2.3210, -1.5985,  0.9085,  1.2390,  2.2919,\n",
      "         -0.8870,  2.5550,  0.8409,  0.8943, -1.2516, -2.1674, -0.9994,  1.2998,\n",
      "          2.2562,  0.8593, -0.6683,  0.3298,  3.2986,  0.4097,  1.2702,  0.2691,\n",
      "         -2.6738,  0.8278,  0.2319,  1.3409,  1.5672, -1.2236,  0.5193,  0.0585],\n",
      "        [ 0.1896, -0.1596,  1.0429, -1.5166, -0.4200,  2.2973,  2.1719,  0.7192,\n",
      "         -0.5162,  2.4166,  1.8897,  0.4072, -1.3310, -1.9191, -1.9621,  0.8644,\n",
      "          1.5880,  0.5864, -1.7263, -0.0420,  2.5340, -1.5941,  1.8585,  1.6045,\n",
      "         -2.6654,  1.1989,  0.1617,  0.7946,  1.2572, -2.1432,  0.7005, -0.6581],\n",
      "        [-0.3373,  0.0151,  1.9853, -2.3552, -1.2441,  0.6865,  0.9093,  1.2820,\n",
      "         -0.4512,  1.7876,  1.0648, -0.1178, -1.2242, -2.3798, -1.7558,  2.1364,\n",
      "          1.2388, -0.3638, -1.4595, -1.2175,  2.9194, -0.9612,  1.1838,  1.3602,\n",
      "         -1.8152,  1.3425,  0.2758,  1.9790,  1.8440, -1.6101,  0.8810, -0.5986],\n",
      "        [ 0.1229,  0.8141,  1.6083, -1.3005, -1.7735,  0.7249,  1.6136,  2.4459,\n",
      "         -0.1789,  1.9311,  1.5739, -0.3293, -0.0161, -2.2011, -1.7130,  1.5775,\n",
      "          0.4814, -0.8574, -1.3622,  0.4208,  2.6253, -0.3559,  1.2936,  1.3317,\n",
      "         -1.5783,  0.4053,  0.3713,  2.0353,  1.0722, -2.2285,  1.6471,  0.2836],\n",
      "        [-1.0634,  1.3340,  0.1760, -2.6111, -1.8912,  1.2381, -0.0270,  1.0302,\n",
      "          0.3014,  2.1833,  0.9098,  1.0808, -1.1328, -2.2355, -2.5212,  1.5167,\n",
      "          0.2599,  0.7528, -0.4305, -1.1135,  2.7632, -2.8572,  0.7914,  0.1025,\n",
      "         -1.4781,  2.5810,  0.3687,  1.2855,  0.4219, -1.4445,  1.5847, -0.9589],\n",
      "        [ 0.0612,  1.1479,  2.4281, -1.4819, -1.5783,  0.2872,  1.2970,  1.4986,\n",
      "         -0.5773,  2.2492,  0.6481,  0.7637,  0.1361, -0.9212, -1.6439,  1.3407,\n",
      "          1.4026, -0.0058, -0.8992,  0.1103,  1.4465, -1.1134,  1.0426, -0.1034,\n",
      "         -1.9581,  0.1728,  0.3645,  1.2230,  1.6791, -1.3325,  0.0444,  1.4971],\n",
      "        [ 0.8485,  0.4889,  0.8029, -1.5577, -0.6758,  1.6335,  1.3496,  0.9221,\n",
      "         -1.1303,  0.8205,  0.6989, -1.3802,  0.4038, -2.0287, -0.6486,  0.7675,\n",
      "          1.3951, -1.1340, -1.9020, -1.7088,  1.2841, -0.5935,  2.1954,  1.2655,\n",
      "         -0.9284, -1.1968, -0.3597, -0.8336,  0.4097, -2.3407,  1.5131,  0.4439],\n",
      "        [-1.6704,  0.1857,  0.8673, -1.9800, -0.3798,  1.4993,  1.6889,  2.1743,\n",
      "         -0.4946,  1.2043,  1.2943,  1.7013, -0.7734, -1.2704,  0.0558,  0.3901,\n",
      "          2.2601, -1.4035, -1.5661, -0.1833,  1.5339, -0.7064,  0.6822,  0.5709,\n",
      "         -2.0870,  0.5367,  0.0338,  1.7718,  2.0758, -2.1795,  0.7608, -0.0889],\n",
      "        [-0.8755,  0.7035,  1.3830, -0.9772, -0.0959,  1.1674,  0.9389,  1.7456,\n",
      "         -0.6827,  0.4629,  1.6364, -0.0483, -0.5348, -2.8714, -1.9170,  0.7004,\n",
      "          0.9236, -0.7888, -0.6047, -1.6460,  1.8349, -0.8512,  1.0279,  1.7885,\n",
      "         -2.5626,  1.0597,  0.6062,  0.9574,  0.9206, -1.4706,  0.8512,  0.4800],\n",
      "        [ 0.2398,  0.9507,  1.3286, -1.5262, -0.0199,  1.2773,  1.5540,  0.3246,\n",
      "         -0.5353,  0.6906,  0.0612, -0.2093, -0.0519, -2.5970, -1.5979,  1.1520,\n",
      "          0.5292, -2.5516,  0.1524, -1.3010,  2.0170, -0.3390,  1.1634,  0.5976,\n",
      "         -1.3090,  0.7450, -1.0755,  0.7088,  1.1992, -2.8497,  1.6283, -0.0398]],\n",
      "       grad_fn=<IndexBackward>)\n"
     ]
    }
   ],
   "source": [
    "from typing import Sequence, Tuple\n",
    "\n",
    "def get_top_K(model, uid : int, k : int) -> Tuple[ Sequence[int], Sequence[float],  np.ndarray ] :\n",
    "  #returns iids, their (normalised) scores in descending order, and item emebddings for the top k predictions of the given uid.\n",
    "\n",
    "  from sklearn.preprocessing import minmax_scale\n",
    "\n",
    "  from scipy.stats import rankdata\n",
    "  # get scores from model\n",
    "  scores = model.predict(uid)\n",
    "\n",
    "  # map scores into rank 0..1 over the entire item space\n",
    "  scores = minmax_scale(scores)\n",
    "\n",
    "  #compute their ranks  \n",
    "  ranks = rankdata(-scores)\n",
    "  \n",
    "  # get and filter iids, scores and embeddings\n",
    "  rtr_scores = scores[ranks <= k]\n",
    "  rtr_iids = np.argwhere(ranks <= k).flatten()\n",
    "  if hasattr(model, '_net'):\n",
    "    embs = model._net.item_embeddings.weight[rtr_iids]\n",
    "  else:\n",
    "    # not a model that has any embeddings\n",
    "    embs = np.zeros([k,1])\n",
    "  \n",
    "  # identify correct ordering using numpy.argsort()\n",
    "  ordering = (-1*rtr_scores).argsort()\n",
    "  \n",
    "  #return iids, scores and their embeddings in descending order of score\n",
    "  return rtr_iids[ordering], rtr_scores[ordering], embs[ordering]\n",
    "\n",
    "if BPRMF is not None:\n",
    "  iids, scores, embs = get_top_K(BPRMF, 0, 10)\n",
    "  print(\"Returned iids: %s\" % str(iids))\n",
    "  print(\"Returned scores: %s\" % str(scores))\n",
    "  print(\"Returned embeddings: %s\" % str(embs))\n",
    "else:\n",
    "  print(\"You need to define BPRMF\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DVYiQRYaEh61"
   },
   "source": [
    "## Task 3. Evaluation of Non-personalised Models\n",
    "Implement the following four (non-personalised) baselines for ranking books based on their statistics:\n",
    " - Average rating, obtained from ratings_df, `ratings` column\n",
    " - Number of ratings, obtained from books_df (column `ratings_count`)\n",
    " - Number of 5* ratings, obtained from books_df (column `ratings_5`)\n",
    " - Fraction of 5* ratings, calculated from the two sources of evidence above, i.e (columns  `ratings_5` and `ratings_count`).\n",
    "\n",
    "Evaluate these in terms of MRR using the provided test data. You may use the StaticModel class below. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "Xop8aPfyFucw"
   },
   "outputs": [],
   "source": [
    "class StaticModel:\n",
    "  \n",
    "  def __init__(self, staticscores):\n",
    "    self.numitems = len(staticscores)\n",
    "    #print(self.numitems)\n",
    "    assert isinstance(staticscores, np.ndarray), \"Expected a numpy array\"\n",
    "    assert staticscores.dtype == np.float32 or staticscores.dtype == np.float64, \"Expected a numpy array of floats\"\n",
    "    self.staticscores = staticscores\n",
    "  \n",
    "  def predict(self, uid):\n",
    "    #this model returns the same scores for each user    \n",
    "    return self.staticscores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6R1q-Zm7FVM9",
    "outputId": "1348d077-0b97-48f6-e7ed-032788928f43"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MRR Score of Average rating: 0.015052024168984034\n",
      "MRR Score of Number of ratings: 0.2396001188245477\n",
      "MRR Score of Number of 5 ratings: 0.2409670879930144\n",
      "MRR Score of Fraction(5*and no ratings): 0.03138372904213526\n"
     ]
    }
   ],
   "source": [
    "#Average rating, obtained from ratings_df, ratings column\n",
    "\n",
    "avg_book_rating = ratings_df.groupby(['book_id']).mean() # Average rating by group book_id\n",
    "all_book_array = ratings_df['book_id'].unique() # Get all unique book_id\n",
    "avg_rating_array = np.zeros_like(all_book_array).astype(np.float32) # Declare zeros array for input value\n",
    "\n",
    "for bid in all_book_array:\n",
    "    avg_rating_array[iid_map[bid]] = avg_book_rating.loc[bid]['rating'] # Get average rating value \n",
    "\n",
    "avg_rating_mrr = mrr_score(StaticModel(avg_rating_array), test_dataset, train=rating_dataset, k=100, verbose=False).mean()\n",
    "print(\"MRR Score of Average rating:\", avg_rating_mrr)\n",
    "\n",
    "#Number of ratings, obtained from books_df (column ratings_count)\n",
    "\n",
    "no_rating_array = np.zeros_like(all_book_array).astype(np.float32) # Declare zeros array for input value\n",
    "\n",
    "for bid in all_book_array:\n",
    "    no_rating_array[iid_map[bid]] = books_df[books_df['book_id']==bid]['ratings_count'] # Get rating count value \n",
    "\n",
    "no_rating_mrr = mrr_score(StaticModel(no_rating_array), test_dataset, train=rating_dataset, k=100, verbose=False).mean()\n",
    "print(\"MRR Score of Number of ratings:\", no_rating_mrr)\n",
    "\n",
    "#Number of 5* ratings, obtained from books_df (column ratings_5)\n",
    "\n",
    "no_5_rating_array = np.zeros_like(all_book_array).astype(np.float32) # Declare zeros array for input value\n",
    "\n",
    "for bid in all_book_array:\n",
    "    no_5_rating_array[iid_map[bid]] = books_df[books_df['book_id']==bid]['ratings_5'] # Get 5 star rating value \n",
    "  \n",
    "no_5_rating_mrr = mrr_score(StaticModel(no_5_rating_array), test_dataset, train=rating_dataset, k=100, verbose=False).mean()\n",
    "print(\"MRR Score of Number of 5 ratings:\", no_5_rating_mrr)\n",
    "\n",
    "#Fraction of 5* ratings, calculated from the two sources of evidence above, i.e (columns ratings_5 and ratings_count).\n",
    "\n",
    "books_df['fraction_rating'] = 0 # Declare new columns in dataframe for input value\n",
    "\n",
    "for item in range(len(all_book_array)): # Calculate fraction rating by rating_5 divided rating_count\n",
    "    books_df.loc[item, 'fraction_rating'] = (books_df['ratings_5'][item] / books_df['ratings_count'][item])\n",
    "\n",
    "frac_rating = books_df['fraction_rating'].values.astype(np.float32) # Get fraction rating value from dataframe\n",
    "\n",
    "frac_rating_mrr = mrr_score(StaticModel(frac_rating), test_dataset, train=rating_dataset, k=100, verbose=False).mean()\n",
    "print(\"MRR Score of Fraction(5*and no ratings):\", frac_rating_mrr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tZqRSixpGvfn"
   },
   "source": [
    "## 4. Qualiatively Examining Recommendations\n",
    "\n",
    "\n",
    "In Recommender Systems, the ground truth (i.e. our list of books that the user has added to their \"to_read\" shelf) can be very incomplete. For instance, this can be because the user is not aware of the book yet.\n",
    "\n",
    "For this reason, it is important to \"eyeball\" the recommendations, to understand what the system is surfacing, and whether the recommendations make sense. In this way, we understand if the recommendations are reasonable, even if they are for books that the user has not actually read according to the test dataset.\n",
    "\n",
    "First, write a function, which given a uid (int), prints the *title and authors* of:\n",
    " - (a) the books that the user has previously shelved (c.f. `toread_dataset`)\n",
    " - (b) the books that the user will read in the future (c.f. `test_dataset`)\n",
    " - (c) the top 10 books that the user were recommended by `BPRMF` - you can make use of `get_top_K()`.\n",
    "\n",
    "You can use the previously defined `getAuthorTitle()` function in your solution.\n",
    "You will also want to compare books in (c) with those in (a) and (b).\n",
    "\n",
    "Then, we will examine two specific users, namely uid 1805 (u336) and uid 179 (user u1331), to analyse if their recommendations make sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Kg1eFa5GYv5c",
    "outputId": "5a3a2ecb-975e-497f-dc05-66d4c52b1dfc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MRR score of uid 1805 (u336)0.41691597413244963\n",
      "The books that the user uid 1805 (u336) has previously shelved.\n",
      "1.Stieg Larsson, Reg Keeland / The Girl Who Kicked the Hornet's Nest (Millennium, #3)\n",
      "2.Suzanne Collins / Mockingjay (The Hunger Games, #3)\n",
      "3.Dennis Lehane / Shutter Island\n",
      "4.Suzanne Collins / Catching Fire (The Hunger Games, #2)\n",
      "5.Paula Hawkins / The Girl on the Train\n",
      "6.Robert Ludlum / The Bourne Supremacy (Jason Bourne, #2)\n",
      "7.John Grisham / The Client\n",
      "8.Thomas Harris / The Silence of the Lambs  (Hannibal Lecter, #2)\n",
      "9.Daphne du Maurier, Sally Beauman / Rebecca\n",
      "10.Robert Ludlum / The Bourne Identity (Jason Bourne, #1)\n",
      "11.Robert Galbraith, J.K. Rowling / The Cuckoo's Calling (Cormoran Strike, #1)\n",
      "12.Stephen King / Misery\n",
      "13.Michael Crichton / Jurassic Park (Jurassic Park, #1)\n",
      "14.Robert Ludlum / The Bourne Ultimatum (Jason Bourne, #3)\n",
      "15.Stephen King, Bernie Wrightson / The Stand\n",
      "16.Michael Crichton / The Andromeda Strain\n",
      "17.Thomas Harris / Red Dragon (Hannibal Lecter, #1)\n",
      "18.Lee Child / Die Trying (Jack Reacher, #2)\n",
      "19.Lee Child / Worth Dying For (Jack Reacher, #15)\n",
      "20.Lee Child / Tripwire  (Jack Reacher, #3)\n",
      "21.Michael Crichton / Congo\n",
      "22.Lee Child, Dick Hill / Without Fail (Jack Reacher, #6)\n",
      "23.Michael Crichton / The Lost World (Jurassic Park, #2)\n",
      "24.Janet Evanovich / One for the Money (Stephanie Plum, #1)\n",
      "25.Tom Clancy / Patriot Games (Jack Ryan Universe, #2)\n",
      "26.Lee Child / Running Blind (Jack Reacher, #4)\n",
      "27.Ken Follett / Eye of the Needle\n",
      "28.Michael Crichton / State of Fear\n",
      "29.Scott Turow / Presumed Innocent\n",
      "30.Harlan Coben / Tell No One\n",
      "-------------------------------------------------------------\n",
      "The books that the user uid 1805 (u336) will read in the future.\n",
      "1.John Grisham / The Pelican Brief\n",
      "2.Stieg Larsson, Reg Keeland / The Girl Who Played with Fire (Millennium, #2)\n",
      "3.Gillian Flynn / Gone Girl\n",
      "4.Tom Clancy / The Hunt for Red October (Jack Ryan Universe, #4)\n",
      "5.Chuck Palahniuk / Fight Club\n",
      "6.Umberto Eco, William Weaver, Seán Barrett / The Name of the Rose\n",
      "7.John Grisham / The Runaway Jury\n",
      "8.Thomas Harris / Hannibal (Hannibal Lecter, #3)\n",
      "9.Lee Child / The Affair (Jack Reacher, #16)\n",
      "10.John Grisham / The Firm (Penguin Readers, Level 5)\n",
      "11.Lee Child / Killing Floor (Jack Reacher, #1)\n",
      "12.John Grisham / A Time to Kill\n",
      "13.Stephen King / The Shining (The Shining #1)\n",
      "14.Michael Crichton / Timeline\n",
      "15.Michael Crichton / Prey\n",
      "16.Jeffery Deaver / The Bone Collector (Lincoln Rhyme, #1)\n",
      "-------------------------------------------------------------\n",
      "The top 10 books that the user uid 1805 (u336) were recommended by BPRMF.\n",
      "1.Scott Turow / Presumed Innocent\n",
      "2.Lee Child / Running Blind (Jack Reacher, #4)\n",
      "3.Lee Child, Dick Hill / Without Fail (Jack Reacher, #6)\n",
      "4.John Grisham / The Firm (Penguin Readers, Level 5)\n",
      "5.Lee Child / Killing Floor (Jack Reacher, #1)\n",
      "6.John Grisham / The Client\n",
      "7.John Grisham / The Rainmaker\n",
      "8.Michael Connelly / The Lincoln Lawyer (Mickey Haller, #1; Harry Bosch Universe, #16)\n",
      "9.John Grisham / The King of Torts\n",
      "10.John Grisham / The Pelican Brief\n",
      "-------------------------------------------------------------\n",
      "Comparing recommend books list with previous and future list\n",
      "[PREVIOUS] 1.Scott Turow / Presumed Innocent\n",
      "[PREVIOUS] 2.Lee Child / Running Blind (Jack Reacher, #4)\n",
      "[PREVIOUS] 3.Lee Child, Dick Hill / Without Fail (Jack Reacher, #6)\n",
      "[FUTURE] 4.John Grisham / The Firm (Penguin Readers, Level 5)\n",
      "[FUTURE] 5.Lee Child / Killing Floor (Jack Reacher, #1)\n",
      "[PREVIOUS] 6.John Grisham / The Client\n",
      "[RECOMMEND] 7.John Grisham / The Rainmaker\n",
      "[RECOMMEND] 8.Michael Connelly / The Lincoln Lawyer (Mickey Haller, #1; Harry Bosch Universe, #16)\n",
      "[RECOMMEND] 9.John Grisham / The King of Torts\n",
      "[FUTURE] 10.John Grisham / The Pelican Brief\n"
     ]
    }
   ],
   "source": [
    "def recommend_read(uid : int):\n",
    "  user_prev_pos = np.argwhere(toread_dataset.user_ids==uid) # Get position of user previously read books by search uid in toread_dataset\n",
    "  user_prev_book = [item[0] for item in toread_dataset.item_ids[user_prev_pos]] # Get list of book_id by using position of user previously read books\n",
    "  prev_list = list()\n",
    "\n",
    "  print(\"The books that the user uid \" + str(uid) + \" (\"+ uid_rev_map.get(uid) +\") has previously shelved.\")\n",
    "  for index, bid in enumerate(user_prev_book):\n",
    "    print(str(index+1) + \".\" + getAuthorTitle(bid))\n",
    "    prev_list.append(getAuthorTitle(bid)) # Store author and title of books that user previously read\n",
    "\n",
    "  print(\"-------------------------------------------------------------\")\n",
    "\n",
    "  user_future_pos = np.argwhere(test_dataset.user_ids==uid) # Get position of user that will read books in the future by search uid in test_dataset\n",
    "  user_future_book = [item[0] for item in test_dataset.item_ids[user_future_pos]] # Get list of book_id by using position of user that will read books in the future\n",
    "  future_list = list()\n",
    "\n",
    "  print(\"The books that the user uid \" + str(uid) + \" (\"+ uid_rev_map.get(uid) +\") will read in the future.\")\n",
    "  for index, bid in enumerate(user_future_book):\n",
    "    print(str(index+1) + \".\" + getAuthorTitle(bid))\n",
    "    future_list.append(getAuthorTitle(bid)) # Store author and title of books that user will read in the future\n",
    "\n",
    "  print(\"-------------------------------------------------------------\")\n",
    "  iids, scores, embs = get_top_K(BPRMF, uid, 10) # Get top 10 books that user were recommend by BPRMF\n",
    "  rec_list = list()\n",
    "\n",
    "  print(\"The top 10 books that the user uid \" + str(uid) + \" (\"+ uid_rev_map.get(uid) +\") were recommended by BPRMF.\")\n",
    "  for index, bid in enumerate(iids):\n",
    "    print(str(index+1) + \".\" + getAuthorTitle(bid))\n",
    "    rec_list.append(getAuthorTitle(bid)) # Store author and title of books that recommend by BPRMF\n",
    "\n",
    "  print(\"-------------------------------------------------------------\")\n",
    "  print(\"Comparing recommend books list with previous and future list\")\n",
    "\n",
    "  for index, book in enumerate(rec_list):\n",
    "    if book in prev_list:\n",
    "      print(\"[PREVIOUS] \" + str(index+1) + \".\"+ book) # If recommended book already in previous list add [PREVIOUS]\n",
    "    elif book in future_list:\n",
    "      print(\"[FUTURE] \" + str(index+1) + \".\"+ book) # If recommended book is in future list add [FUTURE]\n",
    "    else:\n",
    "      print(\"[RECOMMEND] \" + str(index+1) + \".\"+ book) # If recommended book is not in previous or future list then add [FUTURE]\n",
    "\n",
    "uid_test = 1805\n",
    "uid_mrr = mrr_score(BPRMF, test_dataset, train=rating_dataset, k=100, verbose=False).mean()\n",
    "print(\"MRR score of uid \" + str(uid_test) + \" (\"+ uid_rev_map.get(uid_test) +\")\" + str(uid_mrr))\n",
    "recommend_read(uid_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ES_zHeCkNBeC"
   },
   "source": [
    "# Part-C. Diversity of Recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pvBep-ROHWSX"
   },
   "source": [
    "## 5. Measuring Intra-List Diversity\n",
    "\n",
    "\n",
    "For the BPR implicit factorisation model, implement the Intra-list diversity measure of the top 5 scored items based on their item embeddings in the `BPRMF` model. \n",
    "\n",
    "where:\n",
    " - `top_books` is a list or a Numpy array of iids that have been returned for a particular user. For instance, it can be obtained from `get_top_K()`.\n",
    " - `K` is the number of top-ranked items to consider from `top_books`. \n",
    " - Your implementation should use the item emebddings stored in the `BPRMF` model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4n2vBwcnYuM4",
    "outputId": "0b1c89aa-8d1b-4d59-a290-f1d6be0cc451"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The top 5 books of user uid 1805 (u336) are.\n",
      "1.Scott Turow / Presumed Innocent iid 1078 book_id (b966)\n",
      "2.Lee Child / Running Blind (Jack Reacher, #4) iid 1255 book_id (b1870)\n",
      "3.Lee Child, Dick Hill / Without Fail (Jack Reacher, #6) iid 1136 book_id (b1617)\n",
      "4.John Grisham / The Firm (Penguin Readers, Level 5) iid 51 book_id (b123)\n",
      "5.Lee Child / Killing Floor (Jack Reacher, #1) iid 1251 book_id (b477)\n",
      "The Intra-list diversity measure for user (ILD)1805 (u336) is: 0.5686542078852653\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "def measure_ild(top_books : Sequence[int], K : int=5) -> float:\n",
    "  distance_sum = 0\n",
    "  # Calculate cosine similarity between top_books list\n",
    "  for item1 in range(K-1):\n",
    "    for item2 in range((item1),K): \n",
    "      cosim = nn.functional.cosine_similarity(\n",
    "          BPRMF._net.item_embeddings.weight[top_books[item1]],\n",
    "          BPRMF._net.item_embeddings.weight[top_books[item2]], dim=0)\n",
    "      distance_sum += (1-cosim.item())\n",
    "  # Calculate ILD with K and total distance for all top_books\n",
    "  ILD = ( 2 / ( K * ( K - 1 ) ) ) * distance_sum\n",
    "  return ILD\n",
    "\n",
    "uid = 1805\n",
    "k = 5\n",
    "top_books, scores, embs = get_top_K(BPRMF, uid, k) # Get top_books with input uid and k\n",
    "\n",
    "print(\"The top 5 books of user uid \" + str(uid) + \" (\"+ uid_rev_map.get(uid) +\") are.\")\n",
    "\n",
    "for index, bid in enumerate(top_books):\n",
    "  print(str(index+1) + \".\" + getAuthorTitle(bid) + \" iid \" + str(bid) + \" book_id (\" + iid_rev_map.get(bid) +\")\")\n",
    "user_ild = measure_ild(top_books)\n",
    "print(\"The Intra-list diversity measure for user (ILD)\" + str(uid) + \" (\"+ uid_rev_map.get(uid) +\") is: \" + str(user_ild))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZTc1ABbUX977",
    "outputId": "72787389-0d7a-4e72-e1ae-d2ed6f58f037"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The top 5 books of user uid 179 (u1331) are.\n",
      "1.J.K. Rowling, Mary GrandPré / Harry Potter and the Deathly Hallows (Harry Potter, #7) iid 581 book_id (b25)\n",
      "2.J.K. Rowling, Mary GrandPré / Harry Potter and the Goblet of Fire (Harry Potter, #4) iid 11 book_id (b24)\n",
      "3.J.K. Rowling, Mary GrandPré / Harry Potter and the Half-Blood Prince (Harry Potter, #6) iid 189 book_id (b27)\n",
      "4.J.K. Rowling, Mary GrandPré / Harry Potter and the Chamber of Secrets (Harry Potter, #2) iid 10 book_id (b23)\n",
      "5.J.K. Rowling, Mary GrandPré / Harry Potter and the Sorcerer's Stone (Harry Potter, #1) iid 9 book_id (b2)\n",
      "The Intra-list diversity measure for user (ILD)179 (u1331) is: 0.1023072898387909\n"
     ]
    }
   ],
   "source": [
    "uid = 179\n",
    "k = 5\n",
    "top_books, scores, embs = get_top_K(BPRMF, uid, k) # Get top_books with input uid and k\n",
    "\n",
    "print(\"The top 5 books of user uid \" + str(uid) + \" (\"+ uid_rev_map.get(uid) +\") are.\")\n",
    "\n",
    "for index, bid in enumerate(top_books):\n",
    "  print(str(index+1) + \".\" + getAuthorTitle(bid) + \" iid \" + str(bid) + \" book_id (\" + iid_rev_map.get(bid) +\")\")\n",
    "user_ild = measure_ild(top_books)\n",
    "print(\"The Intra-list diversity measure for user (ILD)\" + str(uid) + \" (\"+ uid_rev_map.get(uid) +\") is: \" + str(user_ild))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0qwrP1jUpARF"
   },
   "source": [
    "## 6. Implement MMR Diversification \n",
    "\n",
    "Develop an Maximal Marginal Relevance (M**M**R) diversification technique, to re-rank the top-ranked recommendations for a given user.\n",
    "\n",
    "where iids is a list of iids, scores are their corresponding scores (in descending order), embs is their embeddings, and alpha controls the diversification tradeoff. The function returns a re-ordering of iids. As in previous Exercises, type hints are provided for clarity; a Sequence can be a list or numpy array. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "1VkEMfRvIhKV"
   },
   "outputs": [],
   "source": [
    "from typing import Sequence\n",
    "def mmr(iids : Sequence[int], scores : Sequence[float], embs : np.ndarray, alpha : float) -> Sequence[int]:\n",
    "\n",
    "  assert len(iids) == len(scores)\n",
    "  assert len(iids) == embs.shape[0]\n",
    "  assert len(embs.size()) == 2\n",
    "\n",
    "  #input your solution here returns a re-ordering of iids, such that the first ranked item is first in the list\n",
    "  \n",
    "  # Get first position by finding maximum score\n",
    "  get_max_pos = np.argwhere(scores==np.max(scores))[0] \n",
    "  # Input first position in set\n",
    "  rtr_pos_set = {pos.item() for pos in get_max_pos}\n",
    "  # Input first iids in list\n",
    "  rtr_iids_list = [iids[pos.item()] for pos in get_max_pos]  \n",
    "\n",
    "  for item1 in range(len(iids)-1):\n",
    "    topmmr_pos = -5\n",
    "    topmmr = -5\n",
    "    for item2 in range(len(iids)):\n",
    "      if item2 in rtr_pos_set:\n",
    "        # If target compare item in the set then skip\n",
    "        continue\n",
    "        #print(item2)\n",
    "\n",
    "      max_list = list()\n",
    "      # List of compare between item in set with other item that not in set\n",
    "      for pos in rtr_pos_set:\n",
    "        max_list.append(nn.functional.cosine_similarity(embs[item2], embs[pos], dim=0))\n",
    "      # Get maximum similarity value from list of compare item\n",
    "      maxsim = np.max(np.asarray(max_list))\n",
    "      \n",
    "      # Calculate MMR score for target item\n",
    "      mmr = ( alpha * scores[item2] ) - ( ( 1.0 - alpha ) * maxsim )  \n",
    "\n",
    "      # If MMR score of target item more than current top MMR score then replace\n",
    "      if mmr > topmmr:\n",
    "        topmmr_pos = item2\n",
    "        topmmr = mmr\n",
    "        #print(topmmr_pos)\n",
    "        #print(topmmr)\n",
    "\n",
    "    # After find highest MMR score and their position then put in set and list\n",
    "    if topmmr_pos != -5:\n",
    "      rtr_pos_set.add(topmmr_pos)\n",
    "      rtr_iids_list.append(iids[topmmr_pos])  \n",
    "\n",
    "  #print(rtr_pos_set)\n",
    "  #print(rtr_iids_list)\n",
    "  return rtr_iids_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vfm9mCWZmBPQ"
   },
   "source": [
    "Now we can analyse the impact of our MMR implementation. Let's consider again uid 179 (user u1331). \n",
    "\n",
    "Apply MMR on the top 10 results obtained from the BPRMF model using `get_top_K()`, with an alpha value of 0.5. The following code should help:\n",
    "```python\n",
    "mmr( *get_top_K(bprmodel, 179, 10), 0.5)\n",
    "```\n",
    "\n",
    "Finally, anayse the returned books. Calculate the ILD (with `k=5`), and examine the authors and titles (using `getAuthorTitle()`). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 580,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1wM7m8pOmCnM",
    "outputId": "db7f79a8-21dd-449e-ad59-44db21bdd184"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 result:  [581  11 189  10   9 143   8  33  75  21]\n",
      "ILD of TOP_K:  0.10230576992034912\n",
      "1.J.K. Rowling, Mary GrandPré / Harry Potter and the Deathly Hallows (Harry Potter, #7)\n",
      "2.J.K. Rowling, Mary GrandPré / Harry Potter and the Goblet of Fire (Harry Potter, #4)\n",
      "3.J.K. Rowling, Mary GrandPré / Harry Potter and the Half-Blood Prince (Harry Potter, #6)\n",
      "4.J.K. Rowling, Mary GrandPré / Harry Potter and the Chamber of Secrets (Harry Potter, #2)\n",
      "5.J.K. Rowling, Mary GrandPré / Harry Potter and the Sorcerer's Stone (Harry Potter, #1)\n",
      "------------------------------------------------------------------\n",
      "Top 10 result after MMR [581, 33, 75, 11, 143, 10, 189, 9, 8, 21]\n",
      "ILD of MMR:  0.2888532817363739\n",
      "1.J.K. Rowling, Mary GrandPré / Harry Potter and the Deathly Hallows (Harry Potter, #7) iid 581\n",
      "2.George Orwell / Animal Farm iid 33\n",
      "3.Dan Brown / Angels & Demons  (Robert Langdon, #1) iid 75\n",
      "4.J.K. Rowling, Mary GrandPré / Harry Potter and the Goblet of Fire (Harry Potter, #4) iid 11\n",
      "5.George Orwell, Erich Fromm, Celâl Üster / 1984 iid 143\n"
     ]
    }
   ],
   "source": [
    "#add your solution here\n",
    "\n",
    "top_k = get_top_K(BPRMF, 179, 10)\n",
    "print(\"Top 10 result: \", top_k[0])\n",
    "\n",
    "ild_top_k = measure_ild(top_k[0], K=5)\n",
    "print(\"ILD of TOP_K: \", ild_top_k)\n",
    "\n",
    "for index,mid in enumerate(top_k[0][0:5]):\n",
    "  print(str(index+1) + \".\" + getAuthorTitle(mid))\n",
    "\n",
    "print(\"------------------------------------------------------------------\")\n",
    "\n",
    "top_mmr = mmr( *get_top_K(BPRMF, 179, 10), 0.5)\n",
    "print(\"Top 10 result after MMR\", top_mmr)\n",
    "\n",
    "ild_top_mmr = measure_ild(top_mmr, K=5)\n",
    "print(\"ILD of MMR: \", ild_top_mmr)\n",
    "\n",
    "for index,mid in enumerate(top_mmr[0:5]):\n",
    "  print(str(index+1) + \".\" + getAuthorTitle(mid) + \" iid \" + str(mid))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "THQFNe3zdt1f",
    "49QoGVphnegD",
    "F8N-fEdkMKLD"
   ],
   "name": "Copy of RecSys M Ex3 TEMPLATE.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
